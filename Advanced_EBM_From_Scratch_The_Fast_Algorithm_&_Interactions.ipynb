{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cakirataberk/msc_2025_ataberkcakir_thesis/blob/main/Advanced_EBM_From_Scratch_The_Fast_Algorithm_%26_Interactions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Advanced EBM From Scratch: The Fast Algorithm & Interactions\n",
        "\n",
        "-----\n",
        "\n",
        "## Section 1: Implementation of the Advanced EBM"
      ],
      "metadata": {
        "id": "hPjv4jTBNVOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from interpret.glassbox import ExplainableBoostingRegressor\n",
        "from interpret import show\n",
        "import itertools\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "id": "e4qEJhcvNVOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install interpret"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "76ioPYruNmYq",
        "outputId": "50407dc2-992a-4956-e265-853b92f50810"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting interpret\n",
            "  Downloading interpret-0.7.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting interpret-core==0.7.3 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading interpret_core-0.7.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.25 in /usr/local/lib/python3.12/dist-packages (from interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.19.2 in /usr/local/lib/python3.12/dist-packages (from interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.12/dist-packages (from interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.12/dist-packages (from interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.5.2)\n",
            "Requirement already satisfied: psutil>=5.6.2 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (5.9.5)\n",
            "Requirement already satisfied: ipykernel>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (6.17.1)\n",
            "Requirement already satisfied: ipython>=5.5.0 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (7.34.0)\n",
            "Requirement already satisfied: plotly>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (5.24.1)\n",
            "Collecting SALib>=1.3.3 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading salib-1.5.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: shap>=0.28.5 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.48.0)\n",
            "Requirement already satisfied: dill>=0.2.5 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.3.8)\n",
            "Collecting aplr>=10.6.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading aplr-10.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.6 kB)\n",
            "Collecting dash<3.0.0,>=2.0.0 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading dash-2.18.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting dash-cytoscape>=0.1.1 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading dash_cytoscape-1.0.2.tar.gz (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gevent>=1.3.6 (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading gevent-25.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.12/dist-packages (from interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.32.4)\n",
            "Collecting Flask<3.1,>=1.0.4 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting Werkzeug<3.1 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting dash-html-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting dash-table==5.0.0 (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (8.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.15.0)\n",
            "Collecting retrying (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading retrying-1.4.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (75.2.0)\n",
            "Requirement already satisfied: greenlet>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.2.4)\n",
            "Collecting zope.event (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading zope_event-6.0-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting zope.interface (from gevent>=1.3.6->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading zope_interface-8.0.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.8.15)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (7.4.9)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.1.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (25.0)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (6.4.2)\n",
            "Requirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (5.7.1)\n",
            "Collecting jedi>=0.16 (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.0.52)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19.2->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19.2->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.19.2->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2025.2)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=3.8.1->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (8.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.19.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2025.10.5)\n",
            "Requirement already satisfied: matplotlib>=3.5 in /usr/local/lib/python3.12/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.10.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.70.16)\n",
            "Requirement already satisfied: scipy>=1.9.3 in /usr/local/lib/python3.12/dist-packages (from SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18.1->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.6.0)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.67.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.1.1)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (8.3.0)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.12/dist-packages (from Flask<3.1,>=1.0.4->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.8.5)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.4)\n",
            "Requirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (5.8.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.5->SALib>=1.3.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.2.5)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap>=0.28.5->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.43.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.5.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (0.2.14)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.19.2->interpret-core==0.7.3->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Werkzeug<3.1->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.0.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (3.23.0)\n",
            "Collecting setuptools (from dash<3.0.0,>=2.0.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel>=4.10.0->interpret-core[aplr,dash,debug,linear,notebook,plotly,sensitivity,shap]==0.7.3->interpret) (4.5.0)\n",
            "Downloading interpret-0.7.3-py3-none-any.whl (1.4 kB)\n",
            "Downloading interpret_core-0.7.3-py3-none-any.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aplr-10.16.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash-2.18.2-py3-none-any.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Downloading gevent-25.9.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading salib-1.5.2-py3-none-any.whl (780 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m780.1/780.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.4.2-py3-none-any.whl (10 kB)\n",
            "Downloading zope_event-6.0-py3-none-any.whl (6.4 kB)\n",
            "Downloading setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zope_interface-8.0.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (264 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.7/264.7 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: dash-cytoscape\n",
            "  Building wheel for dash-cytoscape (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for dash-cytoscape: filename=dash_cytoscape-1.0.2-py3-none-any.whl size=4010717 sha256=41c8f0804d3735da316493d96a1c69eb7971057636752deadb5561b47c142cbb\n",
            "  Stored in directory: /root/.cache/pip/wheels/0c/db/f6/9dcb225e9adf45dfef713542769556b1f508170a0759053892\n",
            "Successfully built dash-cytoscape\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, zope.interface, Werkzeug, setuptools, retrying, jedi, aplr, zope.event, Flask, SALib, interpret-core, gevent, dash, dash-cytoscape, interpret\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.2.0\n",
            "    Uninstalling setuptools-75.2.0:\n",
            "      Successfully uninstalled setuptools-75.2.0\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 3.1.2\n",
            "    Uninstalling Flask-3.1.2:\n",
            "      Successfully uninstalled Flask-3.1.2\n",
            "Successfully installed Flask-3.0.3 SALib-1.5.2 Werkzeug-3.0.6 aplr-10.16.0 dash-2.18.2 dash-core-components-2.0.0 dash-cytoscape-1.0.2 dash-html-components-2.0.0 dash-table-5.0.0 gevent-25.9.1 interpret-0.7.3 interpret-core-0.7.3 jedi-0.19.2 retrying-1.4.2 setuptools-80.9.0 zope.event-6.0 zope.interface-8.0.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack"
                ]
              },
              "id": "4a4acccd4933420787b1f2d70a06b082"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Base `ScratchEBM` Class (Fast Algorithm + Interactions)\n",
        "\n",
        "This class is the core of our project. It learns main effects using a custom, fast stump-finding algorithm and then greedily adds interaction terms."
      ],
      "metadata": {
        "id": "cg6HW4L5NVOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import itertools\n",
        "\n",
        "class ScratchEBM:\n",
        "    \"\"\"\n",
        "    The core EBM engine. This final version corrects the interaction gain calculation.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_estimators=2000, learning_rate=0.01, max_bins=1024,\n",
        "                 min_samples_leaf=4, interactions=0, model_type='regression'):\n",
        "        self.n_estimators, self.learning_rate, self.max_bins, self.min_samples_leaf, self.interactions, self.model_type = \\\n",
        "        n_estimators, learning_rate, max_bins, min_samples_leaf, interactions, model_type\n",
        "\n",
        "        self.intercept_ = 0\n",
        "        self.bin_edges_ = []\n",
        "        self.feature_shapes_ = []\n",
        "        self.feature_names_ = []\n",
        "        self.interaction_terms_ = []\n",
        "        self.interaction_models_ = []\n",
        "\n",
        "    def _discretize(self, X):\n",
        "        n_features = X.shape[1]\n",
        "        self.bin_edges_ = [None] * n_features\n",
        "        X_binned = np.zeros_like(X, dtype=int)\n",
        "        for i in range(n_features):\n",
        "            _, self.bin_edges_[i] = np.histogram(X[:, i], bins=self.max_bins)\n",
        "            X_binned[:, i] = np.digitize(X[:, i], self.bin_edges_[i][1:-1])\n",
        "        return X_binned\n",
        "\n",
        "    def _fast_stump_learner(self, y, X_binned_feature):\n",
        "        best_gain, best_split_bin = -np.inf, -1\n",
        "        bin_sums, bin_counts = np.zeros(self.max_bins), np.zeros(self.max_bins)\n",
        "        np.add.at(bin_sums, X_binned_feature, y)\n",
        "        np.add.at(bin_counts, X_binned_feature, 1)\n",
        "\n",
        "        total_sum, total_count = bin_sums.sum(), bin_counts.sum()\n",
        "        current_left_sum, current_left_count = 0, 0\n",
        "\n",
        "        for i in range(self.max_bins - 1):\n",
        "            current_left_sum += bin_sums[i]\n",
        "            current_left_count += bin_counts[i]\n",
        "            if current_left_count < self.min_samples_leaf: continue\n",
        "            current_right_count = total_count - current_left_count\n",
        "            if current_right_count < self.min_samples_leaf: break\n",
        "\n",
        "            gain = (current_left_sum**2 / current_left_count) + ((total_sum - current_left_sum)**2 / current_right_count)\n",
        "            if gain > best_gain:\n",
        "                best_gain, best_split_bin = gain, i\n",
        "\n",
        "        if best_split_bin == -1: return np.zeros(self.max_bins)\n",
        "        update = np.zeros(self.max_bins)\n",
        "        left_indices = np.arange(self.max_bins) <= best_split_bin\n",
        "        right_indices = ~left_indices\n",
        "        left_sum, left_count = bin_sums[left_indices].sum(), bin_counts[left_indices].sum()\n",
        "        right_sum, right_count = bin_sums[right_indices].sum(), bin_counts[right_indices].sum()\n",
        "        update[left_indices] = left_sum / left_count if left_count > 0 else 0\n",
        "        update[right_indices] = right_sum / right_count if right_count > 0 else 0\n",
        "        return update\n",
        "\n",
        "    def fit(self, X, y, X_binned=None, bin_edges=None):\n",
        "        if hasattr(X, 'columns'): self.feature_names_ = X.columns\n",
        "        else: self.feature_names_ = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "\n",
        "        n_features = X.shape[1]\n",
        "        if X_binned is None: X_binned = self._discretize(X)\n",
        "        self.bin_edges_ = bin_edges if bin_edges is not None else self.bin_edges_\n",
        "\n",
        "        self.intercept_ = np.mean(y)\n",
        "        residuals = y.copy() - self.intercept_\n",
        "        self.feature_shapes_ = [np.zeros(self.max_bins) for _ in range(n_features)]\n",
        "\n",
        "        for _ in range(self.n_estimators):\n",
        "            for j in range(n_features):\n",
        "                update = self._fast_stump_learner(residuals, X_binned[:, j])\n",
        "                self.feature_shapes_[j] += self.learning_rate * update\n",
        "                residuals -= self.learning_rate * update[X_binned[:, j]]\n",
        "\n",
        "        if self.interactions > 0:\n",
        "            candidate_pairs = list(itertools.combinations(range(n_features), 2))\n",
        "            for i in range(self.interactions):\n",
        "                # --- BUG FIX: Initialize best_gain to infinity to find the minimum error ---\n",
        "                best_pair, best_model, best_gain = None, None, np.inf\n",
        "                for pair in candidate_pairs:\n",
        "                    j1, j2 = pair\n",
        "                    tree = DecisionTreeRegressor(max_depth=4, min_samples_leaf=self.min_samples_leaf)\n",
        "                    tree.fit(X[:, [j1, j2]], residuals)\n",
        "                    preds = tree.predict(X[:, [j1, j2]])\n",
        "                    # Calculate the new sum of squared errors\n",
        "                    current_sse = np.sum((residuals - preds)**2)\n",
        "                    if current_sse < best_gain:\n",
        "                        best_gain, best_pair, best_model = current_sse, pair, tree\n",
        "\n",
        "                if best_pair is not None:\n",
        "                    # Restored print statement for clarity\n",
        "                    print(f\"Interaction {i+1}: {self.feature_names_[best_pair[0]]} & {self.feature_names_[best_pair[1]]}\")\n",
        "                    self.interaction_terms_.append(best_pair)\n",
        "                    self.interaction_models_.append(best_model)\n",
        "                    candidate_pairs.remove(best_pair)\n",
        "                    residuals -= self.learning_rate * best_model.predict(X[:, [best_pair[0], best_pair[1]]])\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "    def predict(self, X):\n",
        "        preds = np.full(X.shape[0], self.intercept_)\n",
        "        X_binned = np.zeros_like(X, dtype=int)\n",
        "        for i in range(X.shape[1]):\n",
        "             X_binned[:, i] = np.digitize(X[:, i], self.bin_edges_[i][1:-1])\n",
        "\n",
        "        for i in range(X.shape[1]):\n",
        "            preds += self.feature_shapes_[i][X_binned[:, i]]\n",
        "\n",
        "        for pair, model in zip(self.interaction_terms_, self.interaction_models_):\n",
        "            j1, j2 = pair\n",
        "            preds += self.learning_rate * model.predict(X[:, [j1, j2]])\n",
        "\n",
        "        return preds"
      ],
      "metadata": {
        "id": "Uv1vlH4rtmIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The `ScratchEBMWithBagging` Class\n",
        "\n",
        "This class acts as a wrapper. It trains multiple instances of the base `ScratchEBM` and averages their final predictions."
      ],
      "metadata": {
        "id": "1KWBUPynNVOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ScratchEBMWithBagging:\n",
        "    \"\"\"\n",
        "    A wrapper class that correctly uses bagging to stabilize main effects before\n",
        "    finding interaction terms once on the stabilized residuals.\n",
        "    \"\"\"\n",
        "    def __init__(self, ebm_params, outer_bags=8):\n",
        "        self.ebm_params = ebm_params\n",
        "        self.outer_bags = outer_bags\n",
        "        self.final_model = ScratchEBM(**self.ebm_params)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # --- 1. Perform Discretization & Initialize Final Model Attributes ---\n",
        "        print(\"Creating consistent binning for all models...\")\n",
        "        temp_ebm = ScratchEBM(**self.ebm_params)\n",
        "        X_binned_full = temp_ebm._discretize(X)\n",
        "\n",
        "        # --- BUG FIX: Manually initialize bin_edges and feature_names for the final model ---\n",
        "        self.final_model.bin_edges_ = temp_ebm.bin_edges_\n",
        "        # Check if X is a pandas DataFrame to get column names, otherwise create generic names\n",
        "        if hasattr(X, 'columns'):\n",
        "            self.final_model.feature_names_ = X.columns\n",
        "        else:\n",
        "            self.final_model.feature_names_ = [f'feature_{i}' for i in range(n_features)]\n",
        "\n",
        "        # --- 2. Bag the main effects ---\n",
        "        bagged_main_effects = []\n",
        "        base_params_main_only = self.ebm_params.copy()\n",
        "        base_params_main_only['interactions'] = 0\n",
        "\n",
        "        for i in range(self.outer_bags):\n",
        "            print(f\"--- Training Main Effects in Bag {i+1}/{self.outer_bags} ---\")\n",
        "            indices = np.random.choice(n_samples, n_samples, replace=True)\n",
        "            model = ScratchEBM(**base_params_main_only)\n",
        "            # Pass the consistent binning scheme to each model\n",
        "            model.fit(X[indices], y[indices], X_binned=X_binned_full[indices], bin_edges=self.final_model.bin_edges_)\n",
        "            bagged_main_effects.append(model.feature_shapes_)\n",
        "\n",
        "        # --- 3. Average the main effects to get a final, stable model ---\n",
        "        self.final_model.feature_shapes_ = np.mean(bagged_main_effects, axis=0)\n",
        "        self.final_model.intercept_ = np.mean(y)\n",
        "\n",
        "        # --- 4. Calculate stabilized residuals ---\n",
        "        main_effect_preds = np.full(n_samples, self.final_model.intercept_)\n",
        "        for i in range(n_features):\n",
        "            main_effect_preds += self.final_model.feature_shapes_[i][X_binned_full[:, i]]\n",
        "\n",
        "        residuals = y - main_effect_preds\n",
        "\n",
        "        # --- 5. Find interactions ONCE on the stabilized residuals ---\n",
        "        if self.final_model.interactions > 0:\n",
        "            print(\"\\n--- Finding Interactions on Stabilized Residuals ---\")\n",
        "            candidate_pairs = list(itertools.combinations(range(n_features), 2))\n",
        "            for i in range(self.final_model.interactions):\n",
        "                best_pair, best_model, best_gain = None, None, np.inf\n",
        "                for pair in candidate_pairs:\n",
        "                    j1, j2 = pair\n",
        "                    tree = DecisionTreeRegressor(max_depth=4, min_samples_leaf=self.final_model.min_samples_leaf)\n",
        "                    tree.fit(X[:, [j1, j2]], residuals)\n",
        "                    preds = tree.predict(X[:, [j1, j2]])\n",
        "                    current_sse = np.sum((residuals - preds)**2)\n",
        "                    if current_sse < best_gain:\n",
        "                         best_gain, best_pair, best_model = current_sse, pair, tree\n",
        "\n",
        "                if best_pair is not None:\n",
        "                    print(f\"Interaction {i+1}: {self.final_model.feature_names_[best_pair[0]]} & {self.final_model.feature_names_[best_pair[1]]}\")\n",
        "                    self.final_model.interaction_terms_.append(best_pair)\n",
        "                    self.final_model.interaction_models_.append(best_model)\n",
        "                    candidate_pairs.remove(best_pair)\n",
        "                    residuals -= self.final_model.learning_rate * best_model.predict(X[:, [best_pair[0], best_pair[1]]])\n",
        "                else:\n",
        "                    break\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.final_model.predict(X)"
      ],
      "metadata": {
        "id": "FK15L-p8trsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----\n",
        "\n",
        "## Section 2: Benchmarking and Interaction Comparison\n",
        "\n",
        "\n",
        "### Load and Prepare Data"
      ],
      "metadata": {
        "id": "jUt47h2UNVOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and split data\n",
        "housing = fetch_california_housing(as_frame=True)\n",
        "X, y = housing.data, housing.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Use .values to pass NumPy arrays\n",
        "X_train_np, X_test_np = X_train.values, X_test.values\n",
        "y_train_np, y_test_np = y_train.values, y_test.values"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "zN7PyVoiNVOY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Hyperparameter Tuning for `ScratchEBM`\n",
        "\n",
        "Let's run a small grid search to see how performance changes with different numbers of estimators and learning rates."
      ],
      "metadata": {
        "id": "FAP4xkczNVOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [1000, 2000],\n",
        "    'learning_rate': [0.05, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "print(\"--- Starting Hyperparameter Search for ScratchEBM (Regression) ---\")\n",
        "for n_estimators in param_grid['n_estimators']:\n",
        "    for lr in param_grid['learning_rate']:\n",
        "        start_time = time.time()\n",
        "        print(f\"\\nTesting: n_estimators={n_estimators}, learning_rate={lr}\")\n",
        "        model = ScratchEBM(\n",
        "            n_estimators=n_estimators,\n",
        "            learning_rate=lr,\n",
        "            interactions=0  # No interactions for this test\n",
        "        )\n",
        "        model.fit(X_train_np, y_train_np)\n",
        "        preds = model.predict(X_test_np)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test_np, preds))\n",
        "        print(f\"-> RMSE: {rmse:.4f} (took {time.time() - start_time:.2f}s)\")\n",
        "\n",
        "#print(\"\\n--- interpretML EBM finding top 5 interactions ---\")\n",
        "iml_ebm_interactions = ExplainableBoostingRegressor(\n",
        "    random_state=42,\n",
        "    interactions=0 # Also ask for 5 pairs\n",
        ")\n",
        "iml_ebm_interactions.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# Calculate and print final RMSE for interpretML\n",
        "preds_iml = iml_ebm_interactions.predict(X_test)\n",
        "rmse_iml = np.sqrt(mean_squared_error(y_test, preds_iml))\n",
        "print(f\"\\nInterpretML Final RMSE without interactions: {rmse_iml:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Hyperparameter Search for ScratchEBM (Regression) ---\n",
            "\n",
            "Testing: n_estimators=1000, learning_rate=0.05\n",
            "-> RMSE: 0.5989 (took 24.21s)\n",
            "\n",
            "Testing: n_estimators=1000, learning_rate=0.01\n",
            "-> RMSE: 0.6421 (took 23.27s)\n",
            "\n",
            "Testing: n_estimators=1000, learning_rate=0.1\n",
            "-> RMSE: 0.5887 (took 24.33s)\n",
            "\n",
            "Testing: n_estimators=2000, learning_rate=0.05\n",
            "-> RMSE: 0.5888 (took 49.07s)\n",
            "\n",
            "Testing: n_estimators=2000, learning_rate=0.01\n",
            "-> RMSE: 0.6201 (took 48.58s)\n",
            "\n",
            "Testing: n_estimators=2000, learning_rate=0.1\n",
            "-> RMSE: 0.5814 (took 48.89s)\n",
            "\n",
            "InterpretML Final RMSE without interactions: 0.5540\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPdo3I8SNVOY",
        "outputId": "13923ff0-bbb7-4791-bebe-deaccb584eae"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Interaction Term Comparison\n",
        "\n",
        "This is the most interesting part. We will ask both our model and the library to find the top 5 interaction pairs and see if they match."
      ],
      "metadata": {
        "id": "fNGjs-LlNVOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train ScratchEBM to find interactions and compare RMSE ---\n",
        "print(\"\\n--- ScratchEBM finding top 3 interactions ---\")\n",
        "scratch_ebm_interactions = ScratchEBM(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.1,\n",
        "    interactions=3 # Ask the model to find 3 pairs\n",
        ")\n",
        "scratch_ebm_interactions.fit(X_train_np, y_train_np)\n",
        "\n",
        "# Get selected pairs\n",
        "scratch_pairs = [(housing.feature_names[p[0]], housing.feature_names[p[1]]) for p in scratch_ebm_interactions.interaction_terms_]\n",
        "print(\"\\nScratchEBM selected pairs:\", scratch_pairs)\n",
        "\n",
        "# Calculate and print final RMSE for ScratchEBM\n",
        "preds_scratch = scratch_ebm_interactions.predict(X_test_np)\n",
        "rmse_scratch = np.sqrt(mean_squared_error(y_test_np, preds_scratch))\n",
        "print(f\"-> ScratchEBM Final RMSE with interactions: {rmse_scratch:.4f}\\n\")\n",
        "\n",
        "\n",
        "# --- Train interpretML EBM to find interactions and compare RMSE ---\n",
        "print(\"\\n--- interpretML EBM finding top 3 interactions ---\")\n",
        "iml_ebm_interactions = ExplainableBoostingRegressor(\n",
        "    random_state=42,\n",
        "    interactions=3 # Also ask for 3 pairs\n",
        ")\n",
        "iml_ebm_interactions.fit(X_train, y_train)\n",
        "\n",
        "# Get selected pairs\n",
        "iml_global_explanation = iml_ebm_interactions.explain_global()\n",
        "iml_pairs = [term for term in iml_global_explanation.data()['names'] if '&' in term]\n",
        "print(\"\\ninterpretML selected pairs:\", iml_pairs)\n",
        "\n",
        "# Calculate and print final RMSE for interpretML\n",
        "preds_iml = iml_ebm_interactions.predict(X_test)\n",
        "rmse_iml = np.sqrt(mean_squared_error(y_test, preds_iml))\n",
        "print(f\"-> interpretML Final RMSE with interactions: {rmse_iml:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zCh22ZQkBf0",
        "outputId": "17278ad1-f432-4298-cab1-82077d43e411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ScratchEBM finding top 3 interactions ---\n",
            "Finding 3 interaction(s)...\n",
            "Interaction 1: feature_1 & feature_5\n",
            "Interaction 2: feature_5 & feature_7\n",
            "Interaction 3: feature_0 & feature_5\n",
            "\n",
            "ScratchEBM selected pairs: [('HouseAge', 'AveOccup'), ('AveOccup', 'Longitude'), ('MedInc', 'AveOccup')]\n",
            "-> ScratchEBM Final RMSE with interactions: 0.5713\n",
            "\n",
            "\n",
            "--- interpretML EBM finding top 3 interactions ---\n",
            "\n",
            "interpretML selected pairs: ['MedInc & AveOccup', 'HouseAge & AveOccup', 'Latitude & Longitude']\n",
            "-> interpretML Final RMSE with interactions: 0.4765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train ScratchEBM to find interactions and compare RMSE ---\n",
        "print(\"\\n--- ScratchEBM finding top 5 interactions ---\")\n",
        "scratch_ebm_interactions = ScratchEBM(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.1,\n",
        "    interactions=5 # Ask the model to find 5 pairs\n",
        ")\n",
        "scratch_ebm_interactions.fit(X_train_np, y_train_np)\n",
        "\n",
        "# Get selected pairs\n",
        "scratch_pairs = [(housing.feature_names[p[0]], housing.feature_names[p[1]]) for p in scratch_ebm_interactions.interaction_terms_]\n",
        "print(\"\\nScratchEBM selected pairs:\", scratch_pairs)\n",
        "\n",
        "# Calculate and print final RMSE for ScratchEBM\n",
        "preds_scratch = scratch_ebm_interactions.predict(X_test_np)\n",
        "rmse_scratch = np.sqrt(mean_squared_error(y_test_np, preds_scratch))\n",
        "print(f\"-> ScratchEBM Final RMSE with interactions: {rmse_scratch:.4f}\\n\")\n",
        "\n",
        "\n",
        "# --- Train interpretML EBM to find interactions and compare RMSE ---\n",
        "print(\"\\n--- interpretML EBM finding top 5 interactions ---\")\n",
        "iml_ebm_interactions = ExplainableBoostingRegressor(\n",
        "    random_state=42,\n",
        "    interactions=5 # Also ask for 5 pairs\n",
        ")\n",
        "iml_ebm_interactions.fit(X_train, y_train)\n",
        "\n",
        "# Get selected pairs\n",
        "iml_global_explanation = iml_ebm_interactions.explain_global()\n",
        "iml_pairs = [term for term in iml_global_explanation.data()['names'] if '&' in term]\n",
        "print(\"\\ninterpretML selected pairs:\", iml_pairs)\n",
        "\n",
        "# Calculate and print final RMSE for interpretML\n",
        "preds_iml = iml_ebm_interactions.predict(X_test)\n",
        "rmse_iml = np.sqrt(mean_squared_error(y_test, preds_iml))\n",
        "print(f\"-> interpretML Final RMSE with interactions: {rmse_iml:.4f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ScratchEBM finding top 5 interactions ---\n",
            "Finding 5 interaction(s)...\n",
            "Interaction 1: feature_1 & feature_5\n",
            "Interaction 2: feature_5 & feature_7\n",
            "Interaction 3: feature_0 & feature_5\n",
            "Interaction 4: feature_5 & feature_6\n",
            "Interaction 5: feature_2 & feature_5\n",
            "\n",
            "ScratchEBM selected pairs: [('HouseAge', 'AveOccup'), ('AveOccup', 'Longitude'), ('MedInc', 'AveOccup'), ('AveOccup', 'Latitude'), ('AveRooms', 'AveOccup')]\n",
            "-> ScratchEBM Final RMSE with interactions: 0.5674\n",
            "\n",
            "\n",
            "--- interpretML EBM finding top 5 interactions ---\n",
            "\n",
            "interpretML selected pairs: ['MedInc & HouseAge', 'MedInc & AveOccup', 'HouseAge & AveOccup', 'AveOccup & Longitude', 'Latitude & Longitude']\n",
            "-> interpretML Final RMSE with interactions: 0.4737\n"
          ]
        }
      ],
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qxG0PBLNVOZ",
        "outputId": "a61e559c-b19b-42cd-d729-e557e5e585ba"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Common Interactions\n",
        "\n",
        "HouseAge & AveOccup\n",
        "\n",
        "AveOccup & Longitude\n",
        "\n",
        "MedInc & AveOccup"
      ],
      "metadata": {
        "id": "OQdTF4hKpF46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train ScratchEBM to find interactions and compare RMSE ---\n",
        "print(\"\\n--- ScratchEBM finding top 7 interactions ---\")\n",
        "scratch_ebm_interactions = ScratchEBM(\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.1,\n",
        "    interactions=7 # Ask the model to find 7 pairs\n",
        ")\n",
        "scratch_ebm_interactions.fit(X_train_np, y_train_np)\n",
        "\n",
        "# Get selected pairs\n",
        "scratch_pairs = [(housing.feature_names[p[0]], housing.feature_names[p[1]]) for p in scratch_ebm_interactions.interaction_terms_]\n",
        "print(\"\\nScratchEBM selected pairs:\", scratch_pairs)\n",
        "\n",
        "# Calculate and print final RMSE for ScratchEBM\n",
        "preds_scratch = scratch_ebm_interactions.predict(X_test_np)\n",
        "rmse_scratch = np.sqrt(mean_squared_error(y_test_np, preds_scratch))\n",
        "print(f\"-> ScratchEBM Final RMSE with interactions: {rmse_scratch:.4f}\\n\")\n",
        "\n",
        "\n",
        "# --- Train interpretML EBM to find interactions and compare RMSE ---\n",
        "print(\"\\n--- interpretML EBM finding top 7 interactions ---\")\n",
        "iml_ebm_interactions = ExplainableBoostingRegressor(\n",
        "    random_state=42,\n",
        "    interactions=7 # Also ask for 7 pairs\n",
        ")\n",
        "iml_ebm_interactions.fit(X_train, y_train)\n",
        "\n",
        "# Get selected pairs\n",
        "iml_global_explanation = iml_ebm_interactions.explain_global()\n",
        "iml_pairs = [term for term in iml_global_explanation.data()['names'] if '&' in term]\n",
        "print(\"\\ninterpretML selected pairs:\", iml_pairs)\n",
        "\n",
        "# Calculate and print final RMSE for interpretML\n",
        "preds_iml = iml_ebm_interactions.predict(X_test)\n",
        "rmse_iml = np.sqrt(mean_squared_error(y_test, preds_iml))\n",
        "print(f\"-> interpretML Final RMSE with interactions: {rmse_iml:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8dxKkZZmQcz",
        "outputId": "7e799a6d-499f-4274-c805-88723fd046ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- ScratchEBM finding top 7 interactions ---\n",
            "Finding 7 interaction(s)...\n",
            "Interaction 1: feature_1 & feature_5\n",
            "Interaction 2: feature_5 & feature_7\n",
            "Interaction 3: feature_0 & feature_5\n",
            "Interaction 4: feature_5 & feature_6\n",
            "Interaction 5: feature_2 & feature_5\n",
            "Interaction 6: feature_4 & feature_5\n",
            "Interaction 7: feature_3 & feature_5\n",
            "\n",
            "ScratchEBM selected pairs: [('HouseAge', 'AveOccup'), ('AveOccup', 'Longitude'), ('MedInc', 'AveOccup'), ('AveOccup', 'Latitude'), ('AveRooms', 'AveOccup'), ('Population', 'AveOccup'), ('AveBedrms', 'AveOccup')]\n",
            "-> ScratchEBM Final RMSE with interactions: 0.5654\n",
            "\n",
            "\n",
            "--- interpretML EBM finding top 7 interactions ---\n",
            "\n",
            "interpretML selected pairs: ['MedInc & HouseAge', 'MedInc & AveOccup', 'HouseAge & AveOccup', 'Population & AveOccup', 'AveOccup & Latitude', 'AveOccup & Longitude', 'Latitude & Longitude']\n",
            "-> interpretML Final RMSE with interactions: 0.4731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Common Interactions\n",
        "\n",
        "MedInc & AveOccup\n",
        "\n",
        "HouseAge & AveOccup\n",
        "\n",
        "Population & AveOccup\n",
        "\n",
        "AveOccup & Latitude\n",
        "\n",
        "AveOccup & Longitude"
      ],
      "metadata": {
        "id": "eKciNKuNo2ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Part 3: Final Benchmarking with Bagging and Interactions ---\n"
      ],
      "metadata": {
        "id": "cQPMAhFvbMtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Part 3: Final Benchmarking with Bagging and Interactions (Corrected) ---\n",
        "\n",
        "print(\"\\n--- Starting Final Benchmark: Bagged ScratchEBM vs. interpretML ---\")\n",
        "\n",
        "# 1. Define the parameters for the base EBMs\n",
        "ebm_base_params = {\n",
        "    'n_estimators': 2000,\n",
        "    'learning_rate': 0.1,\n",
        "    'max_bins': 1024,\n",
        "    'min_samples_leaf': 4,\n",
        "    'interactions': 5\n",
        "}\n",
        "\n",
        "# 2. Train the ScratchEBM with the correct Bagging architecture\n",
        "print(\"\\n--- Training ScratchEBM with Bagging ---\")\n",
        "scratch_bagged_model = ScratchEBMWithBagging(\n",
        "    ebm_params=ebm_base_params,\n",
        "    outer_bags=14\n",
        ")\n",
        "scratch_bagged_model.fit(X_train_np, y_train_np)\n",
        "\n",
        "# Calculate the final RMSE for the bagged scratch model\n",
        "preds_scratch_bagged = scratch_bagged_model.predict(X_test_np)\n",
        "rmse_scratch_bagged = np.sqrt(mean_squared_error(y_test_np, preds_scratch_bagged))\n",
        "\n",
        "\n",
        "# --- Display the results from the final scratch model ---\n",
        "print(\"\\n--- ScratchEBM (Bagged) Top 5 Interactions ---\")\n",
        "\n",
        "# Get the feature names for easy lookup\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# Access the interaction terms directly from the final_model attribute\n",
        "# No need to loop or count, as interactions are now found once centrally\n",
        "scratch_interactions = scratch_bagged_model.final_model.interaction_terms_\n",
        "for pair_indices in scratch_interactions:\n",
        "    pair_names = (feature_names[pair_indices[0]], feature_names[pair_indices[1]])\n",
        "    print(f\"{pair_names}\")\n",
        "\n",
        "print(f\"\\n-> ScratchEBM (Bagged) Final RMSE: {rmse_scratch_bagged:.4f}\")\n",
        "\n",
        "\n",
        "# 3. Train the interpretML EBM for a final comparison\n",
        "print(\"\\n--- interpretML EBM (with Bagging by Default) ---\")\n",
        "iml_ebm = ExplainableBoostingRegressor(\n",
        "    random_state=42,\n",
        "    interactions=5\n",
        ")\n",
        "iml_ebm.fit(X_train, y_train)\n",
        "\n",
        "# Get selected pairs from the library model\n",
        "iml_global_explanation = iml_ebm.explain_global()\n",
        "iml_pairs = [term for term in iml_global_explanation.data()['names'] if '&' in term]\n",
        "\n",
        "print(\"\\n--- interpretML Top 5 Interactions ---\")\n",
        "print(iml_pairs)\n",
        "\n",
        "# Calculate the final RMSE for the library model\n",
        "preds_iml = iml_ebm.predict(X_test)\n",
        "rmse_iml = np.sqrt(mean_squared_error(y_test, preds_iml))\n",
        "print(f\"\\n-> interpretML Final RMSE: {rmse_iml:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAW_fDaBbIX_",
        "outputId": "c861e111-ac6d-4a42-e24a-0bcb66f21f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Final Benchmark: Bagged ScratchEBM vs. interpretML ---\n",
            "\n",
            "--- Training ScratchEBM with Bagging ---\n",
            "Creating consistent binning for all models...\n",
            "--- Training Main Effects in Bag 1/14 ---\n",
            "--- Training Main Effects in Bag 2/14 ---\n",
            "--- Training Main Effects in Bag 3/14 ---\n",
            "--- Training Main Effects in Bag 4/14 ---\n",
            "--- Training Main Effects in Bag 5/14 ---\n",
            "--- Training Main Effects in Bag 6/14 ---\n",
            "--- Training Main Effects in Bag 7/14 ---\n",
            "--- Training Main Effects in Bag 8/14 ---\n",
            "--- Training Main Effects in Bag 9/14 ---\n",
            "--- Training Main Effects in Bag 10/14 ---\n",
            "--- Training Main Effects in Bag 11/14 ---\n",
            "--- Training Main Effects in Bag 12/14 ---\n",
            "--- Training Main Effects in Bag 13/14 ---\n",
            "--- Training Main Effects in Bag 14/14 ---\n",
            "\n",
            "--- Finding Interactions on Stabilized Residuals ---\n",
            "Interaction 1: feature_1 & feature_5\n",
            "Interaction 2: feature_5 & feature_7\n",
            "Interaction 3: feature_0 & feature_5\n",
            "Interaction 4: feature_5 & feature_6\n",
            "Interaction 5: feature_4 & feature_5\n",
            "\n",
            "--- ScratchEBM (Bagged) Top 5 Interactions ---\n",
            "('HouseAge', 'AveOccup')\n",
            "('AveOccup', 'Longitude')\n",
            "('MedInc', 'AveOccup')\n",
            "('AveOccup', 'Latitude')\n",
            "('Population', 'AveOccup')\n",
            "\n",
            "-> ScratchEBM (Bagged) Final RMSE: 0.5675\n",
            "\n",
            "--- interpretML EBM (with Bagging by Default) ---\n",
            "\n",
            "--- interpretML Top 5 Interactions ---\n",
            "['MedInc & HouseAge', 'MedInc & AveOccup', 'HouseAge & AveOccup', 'AveOccup & Longitude', 'Latitude & Longitude']\n",
            "\n",
            "-> interpretML Final RMSE: 0.4737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# --- Section 4: Visual Comparison of Shape Functions ---\n"
      ],
      "metadata": {
        "id": "ORsxAeZpGAOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n--- Generating Shape Function Comparison Plots ---\")\n",
        "\n",
        "# We already have the trained models:\n",
        "# scratch_bagged_model (your implementation)\n",
        "# iml_ebm (the library's implementation)\n",
        "\n",
        "# We also need the global explanation object from the library model\n",
        "iml_global_explanation = iml_ebm.explain_global()\n",
        "\n",
        "# Get the feature names for plotting\n",
        "feature_names = housing.feature_names\n",
        "\n",
        "# --- Comparison for Feature 1: 'MedInc' ---\n",
        "medinc_index = feature_names.get_loc('MedInc')\n",
        "\n",
        "print(\"\\n--- Feature: MedInc ---\")\n",
        "\n",
        "# 1. Plot from your ScratchEBM (Bagged)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.suptitle(\"Shape Function Comparison for 'MedInc'\", fontsize=16)\n",
        "\n",
        "# Subplot 1: Your Model\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "shape_function = scratch_bagged_model.final_model.feature_shapes_[medinc_index]\n",
        "bin_edges = scratch_bagged_model.final_model.bin_edges_[medinc_index]\n",
        "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "ax1.plot(bin_centers, shape_function)\n",
        "ax1.set_title(\"ScratchEBM (Bagged)\")\n",
        "ax1.set_xlabel(\"MedInc (Median Income)\")\n",
        "ax1.set_ylabel(\"Contribution to Prediction\")\n",
        "ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# 2. Plot from interpretML (extracting data from its explanation)\n",
        "ax2 = plt.subplot(1, 2, 2)\n",
        "medinc_data = iml_global_explanation.data(medinc_index)\n",
        "ax2.plot(medinc_data['names'], medinc_data['scores'])\n",
        "ax2.set_title(\"interpretML Library\")\n",
        "ax2.set_xlabel(\"MedInc (Median Income)\")\n",
        "ax2.set_ylabel(\"Contribution to Prediction\")\n",
        "ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# --- Comparison for Feature 2: 'HouseAge' ---\n",
        "houseage_index = feature_names.get_loc('HouseAge')\n",
        "\n",
        "print(\"\\n--- Feature: HouseAge ---\")\n",
        "\n",
        "# 1. Plot from your ScratchEBM (Bagged)\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.suptitle(\"Shape Function Comparison for 'HouseAge'\", fontsize=16)\n",
        "\n",
        "# Subplot 1: Your Model\n",
        "ax1 = plt.subplot(1, 2, 1)\n",
        "shape_function = scratch_bagged_model.final_model.feature_shapes_[houseage_index]\n",
        "bin_edges = scratch_bagged_model.final_model.bin_edges_[houseage_index]\n",
        "bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n",
        "ax1.plot(bin_centers, shape_function)\n",
        "ax1.set_title(\"ScratchEBM (Bagged)\")\n",
        "ax1.set_xlabel(\"HouseAge\")\n",
        "ax1.set_ylabel(\"Contribution to Prediction\")\n",
        "ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "# 2. Plot from interpretML\n",
        "ax2 = plt.subplot(1, 2, 2)\n",
        "houseage_data = iml_global_explanation.data(houseage_index)\n",
        "ax2.plot(houseage_data['names'], houseage_data['scores'])\n",
        "ax2.set_title(\"interpretML Library\")\n",
        "ax2.set_xlabel(\"HouseAge\")\n",
        "ax2.set_ylabel(\"Contribution to Prediction\")\n",
        "ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vWonFRc-F_4n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}